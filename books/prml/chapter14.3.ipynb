{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14.3 Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "부스팅은 여러 '기저' 분류기들을 조합해서 단일 기저 분류기에 비해 훨씬 더 나은 성능을 보이는 위원회를 구성하는 테크닉이다. <br>\n",
    "부스팅은 (그 어떤 기저 분류기 보다 월등히 좋은 성능을 내는) 위원회를 만드는 방식으로 여러 기저 분류기를 조합하는 강력한 테크닉이다.  <br>\n",
    "Boosting is a powerful technique for combining multiple base classifiers to produce a form of committee whose performance can be significantly better than that of any of the base classifiers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "여기서는 가장 널리 사용되는 부스팅 알고리즘으로서 Freund and Schapire(1996)에 의해 개발된 에이다부스트(AdaBoost)를 살펴볼 것이다. 이름은 적응적 부스팅의 약자다. <br>\n",
    "Here we describe the most widely used form of boosting algorithm called AdaBoost, short for adaptive boosting, developed by Freund and Schapire (1996)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "기저 분류기들이 랜덤보다 아주 약간만 나은 성능을 보이더라도 부스팅을 적용하면 좋은 결과를 얻을 수 있다. 그렇기 때문에 이 경우의 기저 분류기들을 종종 약학습기라 지칭한다. <br>\n",
    "Boosting can give good results even if the base classifiers have a performance that is only slightly better than random, and hense sometimes the base classifiers are known as weak learners."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "부스팅은 원래 분류 문제를 푸는 목적으로 만들어졌지만 회귀 문제를 푸는 데도 활용할 수 있다(Friedman, 2001). <br>\n",
    "Originally designed for solving classification problems, boosting can also be extended to regression (Friedman, 2001)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위에서 살펴본 배깅 등의 위원회 방법과 부스팅 방법 사이의 주된 차이점은 부스팅 방법에서는 기저 분류기들이 순차적으로 훈련된다는 것이다. 이 때 각 기저 분류기들은 가중된 형태의 데이터 집합을 이용해서 훈련되며, 가중 계수들은 이전 분류기의 결과에 의해 결정된다. <br>\n",
    "The principal difference between boosting and the committee methods such as bagging discussed above, is that the base classifiers are trained in sequence, and each base classifier is trained using a weighted form of the data set in which the weighting coefficient associated with each data point depends on the performance of the previous classifiers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "특히, 기본 분류기 중 하나에 의해 잘못 분류 된 포인트는 시퀀스에서 다음 분류기를 훈련하는 데 사용될 때 더 큰 가중치가 부여됩니다. <br>\n",
    "기저 분류기 중 하나에 의해 오분류된 포인트들은 배열상에서의 다음 기저 분류기의 훈련에 사용될 때 더 큰 가중치를 부여받는다. <br>\n",
    "In particular, points that are misclassified by one of the base classifiers are given greater weight when used to train the next classifier in the sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "모든 분류 기가 학습되면 그림 14.1에 개략적으로 설명 된대로 가중치가 적용된 과반수 투표 방식을 통해 예측이 결합됩니다. <br>\n",
    "모든 분류기들을 훈련하고 나면 각 분류기들의 예측치를 가중된 다수결 방식으로 조합한다. 이에 대해 그림 14.1에 그려져 있다. <br>\n",
    "Once all the classifiers have been trained, their predictions are then combined through a weighted majority voting scheme, as illustrated schematically in Figure 14.1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "학습 데이터가 입력 벡터 x1, ..., xN과 해당 바이너리 대상 변수 t1, ..., tN (여기서 tn ∈ {−1, 1})을 포함하는 2- 클래스 분류 문제를 생각해보십시오. <br>\n",
    "2클래스 분류 문제를 고려해 보자. 훈련 데이터는 입력 벡터 x1, ..., xN과 해당 이진 타깃 변수 t1, ..., tN으로 구성되어 있으며, 이 때 $t_n \\in \\{-1, 1\\}$이다. <br>\n",
    "Consider a two-class classification problem, in which the training data comprises input vectors x1,...,xN along with corresponding binary target variables t1,...,tN where $t_N \\in \\{-1, 1\\}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "각 데이터 포인트에는 연관된 가중치 매개 변수 wn이 주어지며, 이는 처음에 모든 데이터 포인트에 대해 1 / N으로 설정됩니다. <br>\n",
    "각 데이터 포인트들은 가중 매개변수 wn을 부여받으며, 매개변수들은 1/N으로 초기화된다. <br>\n",
    "Each data point is given an associated weighting parameter wn, which is initially set 1/N for all data points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "가중 데이터를 사용하여 함수 $y(x) \\in \\{-1, 1\\}$을 제공하는 기본 분류기를 훈련 할 수있는 절차가 있다고 가정합니다. <br>\n",
    "가중된 데이터를 바탕으로 기저 분류기를 훈련시켜서 $y(x) \\in \\{-1, 1\\}$을 내놓을 수 있는 방법은 이미 존재한다고 가정할 것이다. <br>\n",
    "We shall suppose that we have a procedure available for training a base classifier using weighted data to give a function $y(x) \\in \\{-1, 1\\}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "알고리즘의 각 단계에서 AdaBoost는 잘못 분류 된 데이터 포인트에 더 큰 가중치를 부여하기 위해 이전에 훈련 된 분류기의 성능에 따라 가중치 계수가 조정되는 데이터 세트를 사용하여 새로운 분류기를 훈련합니다. <br>\n",
    "에이다부스트의 각 단계에서는 수정된 가중치를 바탕으로 분류기를 훈련시킨다. 이 때 이전에 훈련된 분류기의 결과에서 오분류된 데이터 포인트들에 더 높은 가중치를 부여하는 방식으로 가중치를 수정하게 된다. <br>\n",
    "At each stage of the algorithm, AdaBoost trains a new classifier using a data set in which the weighting coefficiencs are adjusted according to the performance of the previously trained classifers so as to give greater weight to the misclassified data points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "마지막으로 원하는 수의 기본 분류 기가 훈련되면 결합되어 서로 다른 기본 분류기에 다른 가중치를 부여하는 계수를 사용하여위원회를 구성합니다. <br>\n",
    "원하는 수만큼의 기저 분류기를 훈련시키고 나면 이들을 조합해서 위원회를 구성한다. 위원회 구성 가정에서 각 기저 분류기들에 대해 서로 다른 가중치를 사용하게 된다. <br>\n",
    "Finally, when the desired number of base classifiers have been trained, they are combined to form a committee using coefficients that give different weight to different base classifiers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AdaBoost 알고리즘의 정확한 형식은 다음과 같습니다. <br>\n",
    "에이다부스트 알고리즘의 정확한 형태를 다음에 적어 두었다.  <br>\n",
    "The precise form of the AdaBoost alogrithm is given below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AdaBoost\n",
    "1. 데이터 가중치 $\\{w_n\\}$을 $w_n^{(1)} = \\frac{1}{N}$으로 초기화한다. 이 때 n = 1,...,N이다. <br>\n",
    "2. m = 1,...,M에 대해 다음을 시행한다.  <br>\n",
    "    (a) 다음의 가중 오류 함수를 최소화하는 방식으로 분류기 $y_m(x)$를 훈련 데이터에 피팅한다. \n",
    "    $$J_m = \\sum_{n=1}^{N} w_n^{(m)}I(y_m(x_n) \\ne t_n)$$    \n",
    "      여기서 $I(y_m(x_n) \\ne t_n)$은 표시 함수로서 $y_m(x_n) \\ne t_n$이면 1, 아니면 0 값을 가진다. <br>\n",
    "    (b) 다음의 값을 계산한다.  <br>\n",
    "    $$\\epsilon_m = \\frac {\\sum_{n=1}^{N}w_n^{(m)}I(y_m(x_n) \\ne t_n)} {\\sum_{n=1}^{N}w_n^{(m)}} $$\n",
    "    그리고 이를 이용해서 다음을 계산한다.  <br>\n",
    "    $$\\alpha_m = \\ln \\big\\{\\frac{1 - \\epsilon_m} {\\epsilon_m} \\big\\} $$\n",
    "    (c) 데이터 가중 계수를 업데이트한다.  <br>\n",
    "    $$ w_n^{(m+1)} = w_n^{(m)} exp\\{\\alpha_m I(y_m(x_n) \\ne t_n) \\} $$\n",
    "3. 최종 모델을 이용해서 다음과 같이 예측을 시행한다.  <br>\n",
    "$$ Y_M(x) = sign \\big(\\sum_{m=1}^{M} \\alpha_my_m(x)\\big) $$\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AdaBoost <br>\n",
    "1. Initialize the data weighting coefficients $\\{w_n\\}$ by setting $w_n^{(1)} = \\frac{1}{N}$ for $n = 1,\\dots,N$. <br>\n",
    "2. For $m = 1,\\dots,M:$ <br>\n",
    "    (a) Fit a classifier $y_m(x)$ to the training data by minimizing the weighted error function <br>\n",
    "    $$J_m = \\sum_{n=1}^{N} w_n^{(m)}I(y_m(x_n) \\ne t_n)$$    \n",
    "    where $I(y_m(x_n) \\ne t_n)$ is the indicator function and equals 1 when $y_m(x_n) \\ne t_n$ and 0 otherwise. <br>\n",
    "    (b) Evaluate the quantities\n",
    "    $$\\epsilon_m = \\frac {\\sum_{n=1}^{N}w_n^{(m)}I(y_m(x_n) \\ne t_n)} {\\sum_{n=1}^{N}w_n^{(m)}} $$\n",
    "      and then use these to evaluate <br>\n",
    "    $$\\alpha_m = \\ln \\big\\{\\frac{1 - \\epsilon_m} {\\epsilon_m} \\big\\} $$\n",
    "    (c) Update the data weighting coefficients\n",
    "    $$ w_n^{(m+1)} = w_n^{(m)} exp\\{\\alpha_m I(y_m(x_n) \\ne t_n) \\} $$\n",
    "3. Make predictions using the final model, which is given by\n",
    "    $$ Y_M(x) = sign \\big(\\sum_{m=1}^{M} \\alpha_my_m(x)\\big) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "첫 번째 기본 분류기 y1 (x)는 모두 동일한 가중치 계수 wn(1)를 사용하여 훈련되었으므로 단일 분류기를 훈련하기위한 일반적인 절차에 해당합니다. <br>\n",
    "첫 번째 기저 분류기 $y_1(x)$는 모든 가중치 $w_n(1)$ 값이 같은 상태에서 훈련된다. 따라서 단일 분류기를 훈련시키기 위한 보통의 과정을 그대로 따르게 된다. <br>\n",
    "We see that the first base classifier y1(x) is trained using weighting coefficients wn(1), that are all equal, which therefore corresponds to the usual procedure for training a single classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(14.18)에서 우리는 후속 반복에서 가중치 계수 wn(m)가 잘못 분류 된 데이터 포인트에 대해 증가하고 올바르게 분류된 데이터 포인트에 대해 감소한다는 것을 알 수 있습니다. <br>\n",
    "식 14.18을 보면 다음 반복에서부터는 가중 계수 $w_n^{(m)}$의 값이 오분류된 데이터 포인트들에 대해서는 증가하고, 올바르게 분류된 데이터 포인트들에 대해서는 변하지 않는 것을 볼 수 있다. <br>\n",
    "From (14.18), we see that in subsequent iterations the weighting coefficients wn(m) are increased for data points that are misclassified and unchanged for data points that are correctly classified."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "따라서 연속적인 분류자는 이전 분류 자에 의해 잘못 분류 된 포인트를 더 강조해야하며, 연속적인 분류 자에 의해 계속 잘못 분류 된 데이터 포인트는 더 큰 가중치를받습니다. <br>\n",
    "따라서 다음에 연속되는 분류기들은 이전 분류기들에 의해 오분류된 데이터 포인트들을 더 강조해서 훈련을 시행하게 될 것이다. 그리고 계속해서 오분류되는 데이터 포인트들은 더 큰 가중치를 얻게 될 것이다. <br>\n",
    "Successive classifiers are therefore forced to place greater emphasis on points that have been misclassified by previous classifiers, and data points that continue to be misclassified by successive classifiers recieve ever greater weight.\n",
    "Successive classifiers are therefore forced to place greater emphasis on points that have been misclassified by previous classifiers, and data points that continue to be misclassified by successive classifiers receive ever greater weight."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "양 em은 데이터 세트에있는 각 기본 분류기의 오류율에 대한 가중 측정 값을 나타냅니다. <br>\n",
    "$\\epsilon_m$은 각 분류기의 데이터 집합에 대한 가중된 오류율을 나타낸다. <br>\n",
    "The quantities em represent weighted measures of the error rates of each of the base classifiers on the data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "따라서 (14.17)에 의해 정의 된 가중치 계수 αm은 (14.19)에 의해 주어진 전체 출력을 계산할 때 더 정확한 분류기에 더 큰 가중치를 부여합니다. <br>\n",
    "식 14.17에 정의된 가중계수 $\\alpha_m$은 더 정확한 분류기에 대해 더 큰 값을 가지게 될 것이다. 이 값을 바탕으로 식 14.19에서 전체 출력값이 계산된다. <br>\n",
    "We therefore set that the weighting coefficients $\\alpha_m$ defined by (14.17) give greater weight to the more accurate classifiers when computing the overall output given by (14.19)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AdaBoost 알고리즘은 그림 A.7에 표시된 장난감 분류 데이터 집합에서 가져온 30 개 데이터 포인트의 하위 집합을 사용하여 그림 14.2에 나와 있습니다.  <br>\n",
    "에이다부스트 알고리즘의 예시가 그림 14.2에 그려져 있다. 그림 A.7의 분류 데이터 집합에서 30개의 데이터 포인트를 표본 추출해서 사용하였다. <br>\n",
    "The AdaBoost algorithm is illustrated in Figure 14.2, using a subset of 30 data points taken from the toy classification data set shown in Figure A.7."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "여기서 각 기본 학습자는 입력 변수 중 하나에 대한 임계 값으로 구성됩니다. <br>\n",
    "여기서 각각의 기저 학습기는 각각의 입력 변수에 대한 임계값으로 이루어져 있다. <br>\n",
    "Here each base learners consists of a threshold on one of the input variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 간단한 분류기는 '결정 스텀프 (decision stumps)', 즉 단일 노드가있는 결정 트리로 알려진 결정 트리의 한 형태에 해당합니다. <br>\n",
    "이 단순한 분류기는 단일 노드를 가진 의사 결정 트리에 해당한다. 이를 의사 결정 그루터기라 하기도 한다. <br>\n",
    "This simple classifier corresponds to a form of decision tree known as a 'decision stumps', i.e., a decision tree with a single node."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "따라서 각 기본 학습자는 입력 특성 중 하나가 임계 값을 초과하는지 여부에 따라 입력을 분류하므로 공간을 축 중 하나에 평행 한 선형 결정 표면으로 분리 된 두 영역으로 분할합니다. <br>\n",
    "각각의 기저 학습기는 입력 특징들 중 하나가 어떤 임계값을 넘었는지 아닌지를 바탕으로 입력값을 분류한다. 그 결과 ㅇ축들 중 하나에 직교하는 선형 결정 경계를 이용해서 공간을 단순하게 두 구역으로 나누게 된다. <br>\n",
    "Thus each base learner classifies an input according to whether one of the input features exceeds some threshold and therefore simply partitions the space into two regions separated by a linear decision surface that is perpendicular to one of the axes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 14.3.1 지수 오류의 최소화\n",
    "### 14.3.1 Minimizing exponential error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "부스팅은 원래 통계적 학습 이론을 사용하여 동기를 부여하여 일반화 오류의 상한으로 이어졌습니다. <br>\n",
    "부스팅 방법은 원래 통계적 학습 이론에서 기인했으며, 그로 인해서 일반화 오류에 대해 상한 한곗값을 사용한다.  <br>\n",
    "Boosting was originally motivated using statistical learning theory, leading to upper bounds of the generalization error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그러나 이러한 경계는 실제 가치를 갖기에는 너무 느슨하며 부스팅의 실제 성능은 경계 만 제안하는 것보다 훨씬 낫습니다. <br>\n",
    "하지만 이 한곗값들은 실제로 사용하기에는 너무 느슨하다. 즉, 부스팅의 실제적인 성능은 한곗값이 시사하는 것보다 훨씬 좋다는 것이다. <br>\n",
    "However, these bounds turn out to be too loose to have practical value, and the actual performance of Boosting is much better than the bounds alone would suggest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Friedman et al. (2000)은 지수 오차 함수의 순차 최소화 측면에서 부스팅에 대해 다르고 매우 간단한 해석을 제공했습니다.  <br>\n",
    "Friedman et al.(2000)에서는 다른 방식으로 매우 간단하게 부스팅을 해석하는 것에 대해 설명했다. 이 해석은 지수 오류 함수의 연속적인 최소화를 바탕으로 한 것이다. <br>\n",
    "Friedman et al. (2000) gave a different and very simple interpretation of boosting in terms of the sequential minimization of an exponential error function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다음에 의해 정의 된 지수 오류 함수를 고려하십시오. <br>\n",
    "다음과 같이 정의되는 지수 오류 함수를 생각해보자 <br>\n",
    "Consider the exponential error function defined by"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ E = \\sum_{n=1}^{N} exp \\{-t_n f_m(x_n)\\} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "여기서 fm (x)는 다음 형식의 기본 분류기 yl (x)의 선형 조합으로 정의 된 분류기입니다. <br>\n",
    "여기서 fm(x)는 기저 분류기 yl(x)들의 선형 결합을 통해 정의되는 분류기다.  <br>\n",
    "where $f_m(x)$ is a classifier defined in terms of a linear combination of base classifiers $y_l(x)$ of the form"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ f_m(x) = \\frac{1}{2} \\sum_{l=1}^{m} \\alpha_l y_l (x) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tn ∈ {−1, 1}은 훈련 세트 목표 값입니다.  <br>\n",
    "훈련 집합 타깃의 변수들은 tn in {-1, 1}이다.  <br>\n",
    "and tn in {-1, 1} are the training set target values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "우리의 목표는 가중치 계수 α1과 기본 분류기 yl (x)의 매개 변수 모두에 대해 E를 최소화하는 것입니다. <br>\n",
    "우리의 목표는 가중 계수 al과 기저 분류기 yl(x)의 매개 변수에 대해서 E를 최소화하는 것이다.  <br>\n",
    "Our goal is to minimize E with respect to both the weighting coefficients $\\alpha_l$ and the parameter of the base classifiers $y_l(x)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그러나 전역 오류 함수 최소화를 수행하는 대신 기본 분류기 y1 (x),. . . , ym−1 (x)는 계수 α1,. . . , αm−1이므로 αm 및 ym (x)에 대해서만 최소화합니다.  <br>\n",
    "전역적인 오류 함수 최소화를 시행하는 대신에 각각의 기저 분류기 y1(x), ..., ym-1(x)와 해당 계수 a1, ..., am-1 들이 고정되어 있다고 가정하고 am과 ym(x)에 대해서만 최소화를 할 것이다. <br>\n",
    "Instead of doing a global error function minimization, however, we shall suppose that the base classifiers $y_1(x),\\dots,y_{m-1}(x)$ are fixed, as are their coefficients $\\alpha_1,\\dots,\\alpha_{m-1}$, and so we are minimizing only with respect to $\\alpha_m$ and $y_m(x)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "기본 분류 자 ym (x)의 기여도를 분리하면 다음 형식으로 오류 함수를 작성할 수 있습니다. <br>\n",
    "기저 분류기 ym(x)를 포함하는 항들을 제외하면 오류 함수를 다음의 형태로 적을 수 있다. <br>\n",
    "Separating off the contribution from base classifier $y_m(x)$, we can then write the error function in the form."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \n",
    "    E = \\sum_{n=1}^{N}{ exp \\big\\{ -t_n f_{m-1} (x_n) - \\frac{1}{2} t_n \\alpha_m y_m (x_n) \\big \\} } \\\\\n",
    "      = \\sum_{n=1}^{N}{ w_n^{(m)} exp \\big\\{ -\\frac{1}{2} t_n \\alpha_m y_m (x_n) \\big\\} }\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "여기서 계수 w (m) = exp {−t f (x)}는 αm과 ym (x) 만 최적화하기 때문에 상수 n nm-1 n으로 볼 수 있습니다.  <br>\n",
    "여기서 계수 wn(m) = exp{-tnfm-1(xn)}은 상수로 볼 수 있다. 왜냐하면 am과 ym(x)에 대해서만 최적화를 시항할 것이기 때문이다. <br>\n",
    "where the coefficients $w_n(m) = exp\\{-t_n f_{m-1}(x_n)\\}$ can be viewed as constants because we are optimizing only am and ym(x)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ym (x)로 올바르게 분류 된 데이터 포인트 세트를 Tm으로 표시하고 Mm으로 잘못 분류 된 나머지 포인트를 표시하면 차례로 오류 함수를 다음 형식으로 다시 작성할 수 있습니다. <br> \n",
    "$y_m(x)$에 의해 올바르게 분류된 데이터 포인트 집합을 $T_m$으로 지칭하고, 나머지 오분류된 포인트들을 $M_m$으로 지칭하면 오류 함수를 다음의 형태로 다시 적을 수 있다.  <br>\n",
    "\n",
    "If we denote by $\\mathcal{T}_m$ the set of data points that are correctly classified by $y_m(x)$, and if we denote the remaining misclassified points by $\\mathcal{M}_m$, then we can in turn rewrite the error function in the form"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "E = e^{-\\alpha_m/2} \\sum_{n \\in \\mathcal{T}_m} {w_n^{(m)} + e^{\\alpha_m/2}} \\sum_{n \\in \\mathcal{M}_m} {w_n^{(m)}} \\\\\n",
    "  = (e^{\\alpha_m/2} - e^{-\\alpha_m/2}) \\sum_{n=1}^{N} {w_n^{(m)} I (y_m(x_n) \\ne t_n) + e^{-\\alpha_m/2)}}  \\sum_{n=1}^{N}{w_n^{(m)}}\\\\\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ym (x)와 관련하여 이것을 최소화하면 두 번째 항이 상수라는 것을 알 수 있으며, 합산 앞의 전체 곱셈 계수가 위치에 영향을 미치지 않기 때문에 이것은 최소화 (14.15)와 동일합니다.  <br>\n",
    "이를 $y_m(x)$에 대해서 최소화한다고 해보자, 이 경우 두 번째 항은 상수이기 때문에 결국 식 14.15를 최소화하는 것과 동일해진다. 왜냐하면 합산의 앞에 있는 곱셈 인자는 최솟값의 지점에 영향을 주지 않기 때문이다. <br>\n",
    "When we minimize this with respect to $y_m(x)$, we see that the second term is constant, and so this is equivalent to minimizing (14.15) becuase the overall multiplication factor in front of the summation does not affect the location of the minimum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{alignat}{1}\n",
    "f(x^{*}) \\ge f(x^{(k)}) + g^{(k)T}(x^{*}-x^{(k)}) & \\quad \\Longleftrightarrow \\quad \n",
    "f(x^{*})-f(x^{(k)}) \\ge  g^{(k)T}(x^{*}-x^{(k)}) \\\\\n",
    "                     & \\quad  \\Longrightarrow \\quad \n",
    "f(x^{(k)}) - f(x^{*})\\le  g^{(k)T}(x^{(k)}-x^{*}) \\\\\n",
    "                     & \\quad \\Longrightarrow \\quad \n",
    "-2\\alpha_{k}(f(x^{(k)}) - f(x^{*})) \\ge  -2\\alpha_{k}(g^{(k)T}(x^{(k)}-x^{*})) \n",
    "\\end{alignat}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "최소의. 마찬가지로 αm에 대해 최소화하면 m이 (14.16)으로 정의되는 (14.17)을 얻습니다. <br>\n",
    "이와 비슷하게 $\\alpha_m$에 대해서 최소화를 하면 식 14.17을 얻게 된다. 이 때 $\\epsilon_m$은 식 14.16에 의해 정의된다.  <br>\n",
    "Similarly, minimizing with respect to $\\alpha_m$, we obtain (14.17) in which $\\epsilon_m$ is defined by (14.16)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(14.22)에서 αm과 ym (x)를 찾았 으면 데이터 포인트의 가중치가 다음을 사용하여 업데이트됨을 알 수 있습니다. <br>\n",
    "식 14.22로부터 $\\alpha_m$과 $y_m(x)$를 찾고 나면 다음과 같이 데이터의 가중치를 업데이트할 수 있음을 알 수 있다. <br>\n",
    "From (14.22) we see that, having found $a_m$ and $y_m(x)$, the weights on the data points are updated using"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "식 14.25를 대입해 넣으면 식 14.26을 얻을 수 있다.\n",
    "Making use of the fact that"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "가중치 w (m)이 다음 반복에서 다음을 사용하여 업데이트되는 것을 볼 수 있습니다.  <br>\n",
    "식 14.26을 이용해서 가중치 $w_n^{(m)}$을 다음 반복 단계에서 업테이트 할 수 있다.  <br>\n",
    "we see that weights wn(m) are updated at the next iteration using"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "exp (−αm / 2)라는 용어는 n과 무관하기 때문에 모든 데이터 포인트에 동일한 요소에 가중치를 부여하므로 버릴 수 있습니다.  <br>\n",
    "항 $exp(-\\alpha_m/2)$는 n에 대해 독립적이다. 그러므로 이 항은 모든 데이터 포인트들을 같은 인자만큼 가중하게 되며, 따라서 식에서 제거할 수 있다.  <br>\n",
    "Because the term $exp(-\\alpha_m/2)$ is independent of n, we see that it weights all data points by the same factor and so can be discarded."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "따라서 우리는 (14.18)을 얻습니다. <br>\n",
    "그 결과 식 14.18을 얻게 된다.  <br>\n",
    "Thus we obtain (14.18)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "마지막으로 모든 기본 분류 기가 학습되면 (14.21)에 따라 정의 된 결합 함수의 부호를 평가하여 새 데이터 포인트를 분류합니다.  <br>\n",
    "마지막으로, 모든 기저 분류기가 훈련되고 나면 식 14.21의 조합 함수를 통해 계산한 값의 부호에 따라서 새 데이터 포인트들을 분류한다. <br>\n",
    "Finally, once all the base classifiers are trained, new data points are classified by evaluating the sign of the combined function defined according to (14.21)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1/2의 계수는 부호에 영향을주지 않으므로 생략 할 수 있습니다 (14.19). <br>\n",
    "인자 1/2은 부호에 영향을 미치지 않기 때문에 제거할 수 있다. 그 결과 식 14.19를 얻을 수 있다. \n",
    "Because the factor of 1/2 does not affect the sign it can be omitted, giving (14.19)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 부스팅의 오류 함수\n",
    "### Error functions for boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AdaBoost 알고리즘에 의해 최소화되는 지수 오류 함수는 이전 장에서 고려한 것과 다릅니다. <br>\n",
    "에이다부스트 알고리즘에 의해 최소화되는 지수 오류 함수는 이전 장들에서 고려했던 오류 함수들과는 다른 형태를 가진다. <br>\n",
    "The exponential error function that is minimized by the Adaboost algorithm differs from those considered in previous chapters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "지수 오류 함수의 특성에 대한 통찰력을 얻으려면 먼저 다음과 같이 예상되는 오류를 고려합니다. <br>\n",
    "지수 오류 함수의 성질에 대해 통찰을 얻기 위해 먼저 다음의 기대 오륫값을 고려해 보자. <br>\n",
    "To gain some insight into the nature of the exponential error function, we first consider the expected error given by."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\mathbb{E}_{x, t} [exp\\{-ty(x)\\}] = \\sum_t \\int exp\\{-ty(x)\\} p(t|x) p(x) dx $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "가능한 모든 함수 y (x)에 대해 변동 최소화를 수행하면 <br>\n",
    "여기에 모든 가능한 함수 y(x)에 대한 변분적 최소화를 시행하면 다음을 얻게 된다. <br>\n",
    "If we perform a variational minimization with respect to all possible functions y(x), we obtain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ y(x) = \\frac{1}{2} \\ln \\big\\{ \\frac{p(t=1|x)}{p(t=-1|x)} \\big\\} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "로그 확률의 절반입니다. <br>\n",
    "이는 로그 오즈(log odds)의 절반이다. <br>\n",
    "which is half the log odds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "따라서 AdaBoost 알고리즘은 순차 최적화 전략으로 인한 제한적 최소화에 따라 기본 분류기의 선형 조합으로 표현되는 함수 공간 내에서 로그 승산 비에 대한 최상의 근사치를 찾고 있습니다. <br>\n",
    "즉, 에이다부스트 알고리즘은 기저 분류기들의 선형 결합으로 이루어진 함수들의 공간상에서 로그 오즈 비율의 최선의 근사치를 찾는 것이다. 에이다부스트는 순차적 최적화를 통해 시행되므로 이 경우 로그 오즈 비율의 근사치를 찾는 과정은 제약 조건이 있는 최소화에 해당한다. <br>\n",
    "Thus the AdaBoost algorithm is seeking the best approximation to the log odds ratio, within the space of functions represented by the linear combination of base classifiers, subject to the constrained minimization resulting from the sequential optimization strategy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 결과는 (14.19)에서 부호 함수를 사용하여 최종 분류 결정에 도달하도록 동기를 부여합니다. <br>\n",
    "이 결과는 식 14.19에서 부호 함수를 사용해서 최종 분류 결정에 도달하게 되는 이유가 된다. <br>\n",
    "This result motivates the use of the sign function in (14.19) to arrive at the final classification decision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "우리는 이미 2- 클래스 분류에 대한 교차 엔트로피 오차 (4.90)의 최소화 기 y (x)가 사후 클래스 확률에 의해 주어짐을 확인했습니다. <br>\n",
    "2클래스 분류 문제의 경우에 식 4.90의 교차 엔트로피 오류를 최소호하는 y(x)는 사후 클래스 확률로 주어진다는 것을 앞에서 살펴보았다. <br>\n",
    "We have already seen that the minimizer y(x) of the cross-entropy error (4.90) for two-class classification is given by the posterior class probability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "목표 변수 t ∈ {−1, 1}의 경우 오류 함수가 ln (1 + exp (-yt))에 의해 주어짐을 확인했습니다. <br>\n",
    "또한, 타깃 변수 t in {-1, 1}의 경우에는 그 오류 함수가 ln()로 주어진다는 것도 살펴봤었다. <br>\n",
    "In the case of a target variable $t \\in \\{-1, 1\\}$, we have seen that the error function is given by $ln(1 + exp(-yt))$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이것은 그림 14.3의 지수 오차 함수와 비교됩니다. 여기서 교차 엔트로피 오차를 상수 인자 ln (2)로 나누어 비교하기 쉽도록 점 (0, 1)을 통과합니다. <br>\n",
    "그림 14.3에 지수 오류 함수, 교차 엔트로피 오류 함수, 그리고 다른 오류 함수들을 비교해 두었다. 쉬운 비교를 위해서 교차 엔트로피 오류는 상수 인자 ln(2)로 나눠서 포인트 (0, 1)을 지나도록 했다. <br>\n",
    "This is compared with the exponential error function in Figure 14.3, where we have divided the cross-entropy error by a constant factor ln(2) so that it passes through the point (0, 1) for ease of comparison."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "둘 다 이상적인 오 분류 오류 기능에 대한 연속적인 근사치로 볼 수 있습니다. <br>\n",
    "지수 오류 함수와 교차 엔트로피 오류 함수 둘 다가 이상적인 오분류 오류 함수에 대한 연속적 근사치라는 것을 볼 수 있다. <br>\n",
    "We see that both can be seen as continuous approximations to the ideal misclassification error function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "지수 오류의 장점은 순차 최소화가 단순한 AdaBoost 체계로 이어진다는 것입니다. <br>\n",
    "지수 오류 함수는 이를 순차적으로 최소화하면 단순한 에이다부스트 알고리즘을 얻을 수 있다는 장점을 가졌다. <br>\n",
    "An advantage of the exponential error is that its sequential minimization leads to the simple AdaBoost scheme."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그러나 한 가지 단점은 교차 엔트로피보다 훨씬 더 강력하게 ty (x)의 큰 음수 값에 페널티를 준다는 것입니다. <br>\n",
    "반면에 지수 오류 함수는 교차 엔트로피 오류 함수에 비해서 ty(x)의 큰 음의 값에 대해 훨씬 강하게 불이익을 적용한다는 단점을 가지고 있기도 하다. <br>\n",
    "One drawback, however, is that it penalizes large negative values of ty(x) much more strongly than cross-entropy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "특히, ty의 큰 음수 값에 대해 교차 엔트로피는 | ty |와 함께 선형 적으로 증가하는 반면 지수 오류 함수는 | ty |와 함께 기하 급수적으로 증가합니다. <br>\n",
    "ty의 큰 음의 값에 대해서 교차 엔트로피 함수의 값은 |ty|에 대해 선형적으로 증가하는 반면, 지수 오류 함수의 값은 |ty|에 대해 기하급수적으로 증가한다. <br>\n",
    "In particular, we see that for large negative values of ty, the cross-entropy grows linearly with |ty|, whereas the exponential error function grows exponentially with |ty|."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "따라서 예외적 오류 함수는 이상 값이나 잘못 분류 된 데이터 포인트에 대해 훨씬 덜 견고합니다. <br>\n",
    "따라서 지수 오류 함수는 이상점이나 잘못 라벨된 데이터 포인트에 대한 강건성이 훨신 떨어진다. <br>\n",
    "Thus the exponential error function will be much less robust to outliers or misclassified data points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "교차 엔트로피와 지수 오류 함수의 또 다른 중요한 차이점은 후자가 잘 정의 된 확률 모델의 로그 가능도 함수로 해석 될 수 없다는 것입니다. <br>\n",
    "교차 엔트로피 오류 함수와 지수 오류 함수의 또 다른 중요한 차이점은 지수 오류 함수는 잘 정의된 확률 모델의 로그 가능도 함수로 해석할 수가 없다는 것이다. <br>\n",
    "Another important difference between cross-entropy and the exponential error function is that the latter cannont be interpreted as the log likelihood function of any well-defined probabilistic model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "더욱이 지수 오류는 K> 2 클래스를 갖는 분류 문제로 일반화되지 않습니다. 이는 쉽게 일반화되는 확률 모델 (4.108)에 대한 교차 엔트로피와는 대조적입니다. <br>\n",
    "또한, 지수 오류 함수는 K > 2개의 클래스를 가지는 분류 문제에 대해 일반화할 수 없다. 반면에 교차 엔트로피 오류 함수는 잘 정의된 확률적 모델을 가지고 있으며, 쉽게 일반화해서 식 4.108과 같은 결과를 얻을 수 있다. <br>\n",
    "Furthermore, the exponential error does not generalize to classification problems having K > 2 classes, again in contrast to the cross-entropy for a probabilistic model, which is easily generalized to give (4.108)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "지수 오류 (Friedman et al., 2000) 하에서 추가 모델의 순차 최적화로 부스팅을 해석하면 오류 함수 선택을 변경하여 다중 클래스 확장을 포함하여 다양한 부스팅 유사 알고리즘에 대한 문을 열 수 있습니다. <br>\n",
    "부스팅을 지수 오류 함수하에서의 합산 모델의 순차적 최적화로 보는 해석(Friedman et al. 2000)은 많은 종류의 다양한 부스팅 형식 알고리즘의 가능성을 열어준다. 그 중 하나는 다중 클래스에 대해 확장하는 것이다. <br>\n",
    "The interpretation of boosting as the sequential optimization of an additive model under an exponential error (Friedman et al., 2000) opens the door to a wide range of boosting-like algorithms, including multiclass extensions, by altering the choice of error function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "또한 회귀 문제로의 확장에 동기를 부여합니다 (Friedman, 2001). \n",
    "그리고 또 다른 하나는 회귀 문제로의 확장이다 (Friedman, 2001).\n",
    "It also motivates the extension to regression problems (Friedman, 2001)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "회귀에 대한 제곱합 오차 함수를 고려하면 (14.21) 형식의 가법 모델의 순차적 최소화는 단순히 각 새로운 기본 분류기를 이전 모델의 잔차 오차 tn −fm−1 (xn)에 맞추는 것입니다. <br>\n",
    "회귀에 대해 제곱합 오류 함수를 고려하게 되면 식 14.21의 합산 모델의 순차적 최소화 과정은 각각의 새 기저 분류기를 이전 모델의 잔차 오류 tn_fm-1(xn)에 대해 피팅하는 과정이 된다. \n",
    "If we consider a sum-of-squares error function for regression, then sequential optimization of an additive model of the form (14.21) simply involves fitting each new base classifier to the residual errors tn_fm-1(xn) from the previous model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그러나 우리가 언급했듯이 제곱합 오차는 특이 치에 대해 강건하지 않으며 절대 편차 | y − t |에 기반한 부스팅 알고리즘을 사용하여 해결할 수 있습니다. <br>\n",
    "하지만 이미 살펴봤던 것처럼 제곱합 오류 함수는 이상값들에 대해 강건하지 못하다.부스팅 알고리즘을 편차의 절댓값 |y-t|를 기반으로 하도록 하면 이 문제를 해결할 수 있다.  <br>\n",
    "As we have noted, however, the sum-of-squares error is not robust to outliers, and this can be addressed by basing the boosting algorithm on the absolute deviation |y - t| instead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 두 오류 함수는 그림 14.4에서 비교됩니다. <br>\n",
    "두 오류 함수들이 그림 14.4에 비교되어 있다. <br>\n",
    "These two error functions are compared in Figure 14.4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14.4 트리 기반 모델"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "입력 공간을 입방체 영역으로 분할하고 모서리가 축과 정렬 된 다음 각 영역에 간단한 모델 (예 : 상수)을 할당하여 작동하는 간단하지만 널리 사용되는 다양한 모델이 있습니다. <br>\n",
    "간단하지만 널리 사용되는 모델의 종류로 입력 강간을 각 변이 축과 평행한 입방형들로 나누고 각 구역별로 단순한 모델(예를 들면 상수 모델)을 사용하는 것이 있다. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "입력 공간의 특정 지점에서 하나의 모델 만 예측을 수행하는 모델 조합 방법으로 볼 수 있습니다. <br>\n",
    "이러한 모델은 모델 조합 방법의 하나로 볼 수 있다. 이 경우 입력 공간의 각 포인트들에 대해 예측을 하는 데 있어서 오직 하나의 모델만이 책임을 지게 된다. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "새로운 입력 x가 주어지면 특정 모델을 선택하는 프로세스는 이진 트리 (각 노드에서 두 개의 분기로 분할되는 것)의 순회에 해당하는 순차적 의사 결정 프로세스로 설명 할 수 있습니다. <br>\n",
    "새로운 입력값 x가 주어졌을 경우에 특정 모델을 선택하는 과정은 연속적으로 결정을 내리는 과정으로 볼 수 있는데, 이는 이진트리(각 노드에서 두 개의 가지로 쪼개지는 트리)를 순회하는 것에 해당한다. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "여기서는 분류 및 회귀 트리 또는 CART (Breiman et al., 1984)라는 특정 트리 기반 프레임 워크에 초점을 맞춥니다.<br>\n",
    "여기서는 분류와 회귀트리(CART)(Breiman et al., 1984)라 불리는 트리에 초점을 맞출 것이다. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "하지만 ID3 및 C4.5와 같은 이름을 사용하는 다른 변형도 많이 있습니다 (Quinlan, 1986; Quinlan, 1993).<br>\n",
    "여기서는 다루지 않을 것이지만, ID3와 C4.5()와 같은 변형 모델도 존재한다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2",
   "language": "python",
   "name": "tf2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
